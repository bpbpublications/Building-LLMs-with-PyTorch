{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAxu4RcinsTX",
        "outputId": "f79b28e1-715d-42cd-e0e4-4fa53a67f19b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\n"
      ],
      "metadata": {
        "id": "no7P7laNoij0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CUDA not available\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zd5nsCHvo9aK",
        "outputId": "0de898b9-5d46-4427-e06b-faf5b727f90d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3.0+cu121\n",
            "True\n",
            "Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using PyTorch 2.0: Side-by-Side Comparison with Previous Versions"
      ],
      "metadata": {
        "id": "0PQSV6J6e6Z6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic Tensor Operations\n",
        "PyTorch 1.x:"
      ],
      "metadata": {
        "id": "CF2xgKQ3e8xe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Create tensors\n",
        "a = torch.tensor([1, 2, 3])\n",
        "b = torch.tensor([4, 5, 6])\n",
        "\n",
        "# Basic operations\n",
        "c = a + b\n",
        "d = a * b\n",
        "\n",
        "print(f\"a: {a}\")\n",
        "print(f\"b: {b}\")\n",
        "print(f\"a + b: {c}\")\n",
        "print(f\"a * b: {d}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwWzJ2c0eymH",
        "outputId": "ed87853f-0de8-41df-e8db-3e5a6e81fa60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: tensor([1, 2, 3])\n",
            "b: tensor([4, 5, 6])\n",
            "a + b: tensor([5, 7, 9])\n",
            "a * b: tensor([ 4, 10, 18])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch 2.0: In PyTorch 2.0, operations are optimized for GPU (CUDA) execution, enhancing performance."
      ],
      "metadata": {
        "id": "Zg36XyYUfa2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Create tensors\n",
        "a = torch.tensor([1, 2, 3], device='cuda')\n",
        "b = torch.tensor([4, 5, 6], device='cuda')\n",
        "\n",
        "# Basic operations\n",
        "c = a + b\n",
        "d = a * b\n",
        "\n",
        "print(f\"a: {a}\")\n",
        "print(f\"b: {b}\")\n",
        "print(f\"a + b: {c}\")\n",
        "print(f\"a * b: {d}\")"
      ],
      "metadata": {
        "id": "eUvnsKyBfXcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dynamic Shapes and Control Flow\n",
        "PyTorch 1.x:"
      ],
      "metadata": {
        "id": "Ord9bWw5f16n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3, 4)\n",
        "y = torch.randn(4, 5)\n",
        "\n",
        "result = torch.matmul(x, y)\n",
        "print(f\"Result: {result}\")"
      ],
      "metadata": {
        "id": "BemuKUX3f4ZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch 2.0:Dynamic shapes and control flow are better supported and more efficiently executed on GPUs in PyTorch 2.0."
      ],
      "metadata": {
        "id": "N1uidpyaf-jR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3, 4, device='cuda')\n",
        "y = torch.randn(4, 5, device='cuda')\n",
        "\n",
        "result = torch.matmul(x, y)\n",
        "print(f\"Result: {result}\")\n"
      ],
      "metadata": {
        "id": "T0TMxIS0f9IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QJ8y4XM_gDnO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using PyTorch 2.0: Side-by-Side Comparison with Previous Versions\n",
        "To illustrate the differences and improvements in PyTorch 2.0 compared to previous versions, we'll highlight how common operations and tasks have been enhanced. This comparison will help you understand the new features and optimizations in PyTorch 2.0, making it easier to use the latest version effectively.\n",
        "\n",
        "Dynamic Shapes and Control Flow\n",
        "PyTorch 1.x:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import torch\n",
        "\n",
        "# Create tensors on CPU\n",
        "x = torch.randn(3, 4)\n",
        "y = torch.randn(4, 5)\n",
        "\n",
        "# Perform matrix multiplication\n",
        "result = torch.matmul(x, y)\n",
        "print(f\"Result: {result}\")\n",
        "PyTorch 2.0:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import torch\n",
        "\n",
        "# Create tensors on GPU\n",
        "x = torch.randn(3, 4, device='cuda')\n",
        "y = torch.randn(4, 5, device='cuda')\n",
        "\n",
        "# Perform matrix multiplication\n",
        "result = torch.matmul(x, y)\n",
        "print(f\"Result: {result}\")\n",
        "Explanation of Differences\n",
        "Dynamic Shapes and Control Flow\n",
        "In PyTorch 1.x, tensor operations such as matrix multiplication (torch.matmul) can be performed on both CPU and GPU. However, handling variable-length inputs and dynamic shapes can be less efficient and flexible.\n",
        "\n",
        "PyTorch 2.0 Enhancements:\n",
        "\n",
        "Improved GPU Utilization: By creating tensors directly on the GPU (using device='cuda'), PyTorch 2.0 ensures better performance for operations like matrix multiplication. This is particularly beneficial for dynamic shapes and control flow operations that can take advantage of GPU acceleration.\n",
        "\n",
        "Efficient Execution: PyTorch 2.0 introduces enhancements that make the execution of operations involving dynamic shapes and control flow more efficient. This allows models to handle variable-length inputs more effectively, reducing overhead and improving overall performance.\n",
        "\n",
        "Automatic Optimizations: The new TorchInductor backend in PyTorch 2.0 provides automatic optimizations for both CPU and GPU code, which includes handling dynamic shapes more efficiently. This means less manual optimization is required from developers.\n",
        "\n",
        "Mixed Precision Training\n",
        "PyTorch 1.x (with apex):\n",
        "\n",
        "python\n",
        "Copy code\n",
        "from apex import amp\n",
        "import torch\n",
        "\n",
        "model = torch.nn.Linear(4, 4).cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
        "\n",
        "# Dummy data\n",
        "data = torch.randn(16, 4, device='cuda')\n",
        "target = torch.randn(16, 4, device='cuda')\n",
        "\n",
        "output = model(data)\n",
        "loss = torch.nn.functional.mse_loss(output, target)\n",
        "\n",
        "with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "    scaled_loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "print(f\"Output: {output}\")\n",
        "PyTorch 2.0:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import torch\n",
        "\n",
        "model = torch.nn.Linear(4, 4).cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Dummy data\n",
        "data = torch.randn(16, 4, device='cuda')\n",
        "target = torch.randn(16, 4, device='cuda')\n",
        "\n",
        "# Training step with mixed precision\n",
        "with autocast():\n",
        "    output = model(data)\n",
        "    loss = torch.nn.functional.mse_loss(output, target)\n",
        "\n",
        "scaler.scale(loss).backward()\n",
        "scaler.step(optimizer)\n",
        "scaler.update()\n",
        "\n",
        "print(f\"Output: {output}\")\n",
        "Explanation of Differences\n",
        "Mixed Precision Training\n",
        "In PyTorch 1.x, mixed precision training was possible using the NVIDIA apex library, which required additional setup and manual management of mixed precision operations.\n",
        "\n",
        "PyTorch 2.0 Enhancements:\n",
        "\n",
        "Integrated Mixed Precision: PyTorch 2.0 natively supports mixed precision training using the torch.cuda.amp module, eliminating the need for the external apex library. This integration simplifies the process and reduces the potential for errors.\n",
        "\n",
        "Autocast and GradScaler: The autocast context manager and GradScaler in PyTorch 2.0 make it easier to implement mixed precision training. autocast automatically casts operations to the appropriate precision, while GradScaler manages the scaling of gradients to prevent underflow, providing a more user-friendly and efficient approach.\n",
        "\n",
        "Asynchronous CUDA Execution\n",
        "PyTorch 1.x:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "# Asynchronous execution required more manual setup.\n",
        "import torch\n",
        "\n",
        "stream = torch.cuda.Stream()\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "with torch.cuda.stream(stream):\n",
        "    a = torch.rand(10000, 10000, device='cuda')\n",
        "    b = torch.rand(10000, 10000, device='cuda')\n",
        "    c = torch.matmul(a, b)\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "print(\"Matrix multiplication completed asynchronously\")\n",
        "PyTorch 2.0:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import torch\n",
        "\n",
        "stream = torch.cuda.Stream()\n",
        "\n",
        "with torch.cuda.stream(stream):\n",
        "    a = torch.rand(10000, 10000, device='cuda')\n",
        "    b = torch.rand(10000, 10000, device='cuda')\n",
        "    c = torch.matmul(a, b)\n",
        "\n",
        "print(\"Matrix multiplication completed asynchronously\")\n",
        "Explanation of Differences\n",
        "Asynchronous CUDA Execution\n",
        "In PyTorch 1.x, setting up asynchronous execution with CUDA streams required explicit synchronization and careful management to ensure correct execution.\n",
        "\n",
        "PyTorch 2.0 Enhancements:\n",
        "\n",
        "Simplified Asynchronous Execution: PyTorch 2.0 streamlines the setup for asynchronous execution. By improving the integration with CUDA streams, operations can be more easily overlapped without the need for manual synchronization calls (torch.cuda.synchronize), simplifying the code and improving performance.\n",
        "\n",
        "Better Performance: These enhancements reduce idle times and improve the overall performance of GPU computations by allowing better utilization of hardware resources, especially for large-scale tensor operations."
      ],
      "metadata": {
        "id": "mmPA7CzRq20r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from apex import amp\n",
        "import torch\n",
        "\n",
        "model = torch.nn.Linear(4, 4).cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
        "\n",
        "# Dummy data\n",
        "data = torch.randn(16, 4, device='cuda')\n",
        "target = torch.randn(16, 4, device='cuda')\n",
        "\n",
        "output = model(data)\n",
        "loss = torch.nn.functional.mse_loss(output, target)\n",
        "\n",
        "with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "    scaled_loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "print(f\"Output: {output}\")\n"
      ],
      "metadata": {
        "id": "X94Bn4PCgBba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pytorch 2.0 mixed precision training"
      ],
      "metadata": {
        "id": "4Na6FkCWtClL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import torch\n",
        "model = torch.nn.Linear(4, 4).cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "scaler = GradScaler()\n",
        "# Dummy data\n",
        "data = torch.randn(16, 4, device='cuda')\n",
        "target = torch.randn(16, 4, device='cuda')\n",
        "# Training step with mixed precision\n",
        "with autocast():\n",
        "    output = model(data)\n",
        "    loss = torch.nn.functional.mse_loss(output, target)\n",
        "scaler.scale(loss).backward()\n",
        "scaler.step(optimizer)\n",
        "scaler.update()\n",
        "print(f\"Output: {output}\")"
      ],
      "metadata": {
        "id": "EWDzDtYQtFAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Asynchronous CUDA Execution"
      ],
      "metadata": {
        "id": "BL2uEi96ttlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Asynchronous execution required more manual setup.#pytorch 1.x\n",
        "import torch\n",
        "\n",
        "stream = torch.cuda.Stream()\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "with torch.cuda.stream(stream):\n",
        "    a = torch.rand(10000, 10000, device='cuda')\n",
        "    b = torch.rand(10000, 10000, device='cuda')\n",
        "    c = torch.matmul(a, b)\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "print(\"Matrix multiplication completed asynchronously\")\n"
      ],
      "metadata": {
        "id": "DTQFp2WWtvA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pytorch 2.0 Asynchronous execution\n",
        "import torch\n",
        "\n",
        "stream = torch.cuda.Stream()\n",
        "\n",
        "with torch.cuda.stream(stream):\n",
        "    a = torch.rand(10000, 10000, device='cuda')\n",
        "    b = torch.rand(10000, 10000, device='cuda')\n",
        "    c = torch.matmul(a, b)\n",
        "\n",
        "print(\"Matrix multiplication completed asynchronously\")\n"
      ],
      "metadata": {
        "id": "jBHc3KZGt0Nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mixed Precision training Cifar example"
      ],
      "metadata": {
        "id": "Jd_Y_s6-DlTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Data transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Loading CIFAR-10 data\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZm90_BwDp2D",
        "outputId": "ebca51cf-cc1e-43d9-bf4a-138bdeae1198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:05<00:00, 30844359.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(nn.ReLU()(self.conv1(x)))\n",
        "        x = self.pool(nn.ReLU()(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.ReLU()(self.fc1(x))\n",
        "        x = nn.ReLU()(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Bl0u-cPODu6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "model = SimpleCNN().cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "scaler = GradScaler()\n",
        "\n",
        "for epoch in range(2):  # Example with 2 epochs\n",
        "    for i, (inputs, labels) in enumerate(trainloader):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            outputs = model(inputs)\n",
        "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        if i % 200 == 199:\n",
        "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {loss.item():.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmeFIDstDxi9",
        "outputId": "8a92b653-b1a3-400e-fa1b-8336b930e4c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 200, Loss: 2.315\n",
            "Epoch 1, Batch 400, Loss: 2.301\n",
            "Epoch 1, Batch 600, Loss: 2.281\n",
            "Epoch 1, Batch 800, Loss: 2.266\n",
            "Epoch 1, Batch 1000, Loss: 2.223\n",
            "Epoch 1, Batch 1200, Loss: 2.112\n",
            "Epoch 1, Batch 1400, Loss: 2.007\n",
            "Epoch 2, Batch 200, Loss: 1.941\n",
            "Epoch 2, Batch 400, Loss: 1.929\n",
            "Epoch 2, Batch 600, Loss: 1.926\n",
            "Epoch 2, Batch 800, Loss: 1.884\n",
            "Epoch 2, Batch 1000, Loss: 2.011\n",
            "Epoch 2, Batch 1200, Loss: 1.702\n",
            "Epoch 2, Batch 1400, Loss: 1.648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using Torchscript"
      ],
      "metadata": {
        "id": "dl939bdeTrAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.layer1 = nn.Linear(10, 50)\n",
        "        self.layer2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.layer1(x))\n",
        "        x = self.layer2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "__iTt-rmTtOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert Model to TorchScript\n",
        "# Initialize the model\n",
        "model = NeuralNet()\n",
        "\n",
        "# Example input tensor\n",
        "example_input = torch.rand(1, 10)\n",
        "\n",
        "# Trace the model\n",
        "traced_model = torch.jit.trace(model, example_input)\n"
      ],
      "metadata": {
        "id": "_mS8guijTxT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save and Load TorchScript\n",
        "\n",
        "# Save the model\n",
        "traced_model.save(\"model.pt\")\n",
        "\n",
        "# Load the model\n",
        "loaded_model = torch.jit.load(\"model.pt\")\n"
      ],
      "metadata": {
        "id": "GQ5GmQBfT5oP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the model with some input\n",
        "output = loaded_model(torch.rand(1, 10))\n",
        "print(output)"
      ],
      "metadata": {
        "id": "YvXcUNbVW2tp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}